{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello  world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello  world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import IPythonMagic\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray\n",
    "from Timer import Timer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global logger already initialized!\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Context already registered! Ignoring\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(5): warning: variable \"gid\" was declared but never referenced\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernel_src=\"\"\"\n",
    "__global__ void shmemReduction(float* output, float* input, int size) {\n",
    "    //First we strike through global memory and compute the maximum for every thread\n",
    "    int gid=blockIdx.x*blockDim.x+threadIdx.x; //blockIdx.x is always zero because we use just one block\n",
    "    \n",
    "    float max_value = -999999.99; //FIX ME \n",
    "    for (int i=threadIdx.x; i<size; i=i+blockDim.x ) {\n",
    "        max_value=fmaxf(max_value,input[i]);\n",
    "        \n",
    "    \n",
    "    }\n",
    "    //Temporary write to check things\n",
    "    output[threadIdx.x]=max_value;\n",
    "    \n",
    "    //Store the per-thread maximum in shared memory\n",
    "    __shared__ float max_shared[128];\n",
    "    max_shared[threadIdx.x]=max_value;\n",
    "    \n",
    "    //Synchronize so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "    \n",
    "    //Find the maximum in shared memory\n",
    "    //Reduce from 128 to 64 elements\n",
    "    if (threadIdx.x<64) {\n",
    "            max_shared[threadIdx.x]=fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x+64]);\n",
    "    }     \n",
    "    \n",
    "     __syncthreads(); //Since we have more than 1 warp we must sychronize all threads \n",
    "    //Reduce from 64 to 32 elements\n",
    "    if (threadIdx.x<32) {\n",
    "            max_shared[threadIdx.x]=fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x+32]);\n",
    "    }    \n",
    "    \n",
    "    \n",
    "    //Reduce from 32 to 16 elements\n",
    "    if (threadIdx.x<16) {\n",
    "            max_shared[threadIdx.x]=fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x+16]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 16 to 8 elements\n",
    "    if (threadIdx.x<8) {\n",
    "            max_shared[threadIdx.x]=fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x+8]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 8 to 4 elements\n",
    "    if (threadIdx.x<4) {\n",
    "            max_shared[threadIdx.x]=fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x+8]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 4 to 2 elements\n",
    "    if (threadIdx.x<2) {\n",
    "            max_shared[threadIdx.x]=fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x+4]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 2 to 1 elements\n",
    "    if (threadIdx.x<1) {\n",
    "            max_shared[threadIdx.x]=fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x+1]);\n",
    "    }\n",
    "    \n",
    "    //Finally write out to out\n",
    "    if (threadIdx.x==0) {\n",
    "        output[0]=max_shared[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "kernel_module=cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function=kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=256\n",
    "a=np.random.random((1,n)).astype(np.float32)\n",
    "a_g=GPUArray(a.shape,a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads=128\n",
    "b=np.empty((1,num_threads)).astype(np.float32)\n",
    "b_g=GPUArray(b.shape,b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32190987 0.73669046 0.79639834 0.5749647  0.37169003 0.93374985\n",
      "  0.13384005 0.14896119 0.06422897 0.27464053 0.5667491  0.82810557\n",
      "  0.15946524 0.5317411  0.2200729  0.5245224  0.17068164 0.81901866\n",
      "  0.15178077 0.09561454 0.71968585 0.8010821  0.41355345 0.91683173\n",
      "  0.8387901  0.26513612 0.50686264 0.8826764  0.21542163 0.5074297\n",
      "  0.8487877  0.24108186 0.7996128  0.42004842 0.02840959 0.82682496\n",
      "  0.96313024 0.445275   0.5112033  0.33561134 0.1547946  0.8641053\n",
      "  0.08838812 0.37754104 0.5601205  0.84132665 0.49553826 0.30761972\n",
      "  0.7204642  0.23711719 0.09768553 0.25728643 0.42013776 0.253011\n",
      "  0.18745714 0.7056812  0.569525   0.42929474 0.6766775  0.08781328\n",
      "  0.9807683  0.413401   0.64580375 0.42392704 0.10422944 0.17686765\n",
      "  0.00446184 0.908137   0.88811606 0.4777462  0.5021096  0.04944073\n",
      "  0.728923   0.94791824 0.41820842 0.39544752 0.01220619 0.16160762\n",
      "  0.3169896  0.36709872 0.21053743 0.7822698  0.35975596 0.69406986\n",
      "  0.4851765  0.65120757 0.98760766 0.801135   0.84763056 0.76043993\n",
      "  0.00200785 0.5790382  0.44322136 0.2801771  0.19964156 0.1663507\n",
      "  0.13051736 0.4987074  0.30100068 0.21077278 0.9454533  0.01682778\n",
      "  0.51662886 0.9112555  0.66190076 0.89619136 0.682532   0.03120061\n",
      "  0.65369576 0.9558269  0.9033645  0.6244497  0.89265597 0.3480445\n",
      "  0.91517496 0.9794953  0.7174799  0.8004315  0.8671144  0.66151905\n",
      "  0.91528964 0.4977642  0.47784844 0.01302483 0.4801533  0.28522992\n",
      "  0.38551193 0.04318088 0.1807604  0.29432255 0.3319777  0.16156009\n",
      "  0.47253448 0.6569601  0.13862768 0.60355026 0.5874686  0.9990609\n",
      "  0.6995461  0.39588162 0.72730035 0.46736702 0.5928625  0.7804733\n",
      "  0.74312484 0.15889235 0.2929889  0.6727449  0.2343862  0.50109535\n",
      "  0.28086248 0.8869599  0.23397234 0.6045625  0.8842177  0.51885974\n",
      "  0.25480294 0.1425491  0.3992132  0.1429271  0.90038013 0.41332138\n",
      "  0.7507496  0.1768708  0.51342255 0.3004523  0.7538969  0.8853897\n",
      "  0.44888878 0.17433912 0.27045235 0.07265741 0.5932086  0.73220265\n",
      "  0.8154106  0.07373114 0.4345457  0.92849857 0.45925483 0.91614395\n",
      "  0.78438455 0.57453805 0.87220395 0.11698986 0.90794915 0.9854671\n",
      "  0.90975046 0.81758285 0.21361926 0.9888912  0.99123645 0.79706883\n",
      "  0.9304855  0.56574327 0.3646053  0.94114697 0.9322931  0.9603143\n",
      "  0.7784568  0.58973473 0.7924536  0.10714328 0.7095202  0.33741063\n",
      "  0.6771098  0.8434838  0.6051711  0.8523366  0.08525497 0.5770447\n",
      "  0.71769166 0.11312585 0.20762977 0.98365533 0.36438164 0.3791656\n",
      "  0.858436   0.15969422 0.8986275  0.79017204 0.53074104 0.61446065\n",
      "  0.65697587 0.9587186  0.81116664 0.3442356  0.7209571  0.8171486\n",
      "  0.86900187 0.7813769  0.9524976  0.34355265 0.2547274  0.4938356\n",
      "  0.9651772  0.24486865 0.6271279  0.09919292 0.78949    0.44635895\n",
      "  0.5391631  0.23837124 0.34682155 0.51875424 0.18800318 0.6761934\n",
      "  0.51993775 0.98788255 0.25344262 0.7170477  0.36208957 0.3987279\n",
      "  0.4113548  0.6656881  0.7383772  0.8727519 ]]\n",
      "[[0.9990609  0.73669046 0.79639834 0.5749647  0.47253448 0.93374985\n",
      "  0.13862768 0.60355026 0.5874686  0.9990609  0.6995461  0.82810557\n",
      "  0.72730035 0.5317411  0.5928625  0.7804733  0.74312484 0.81901866\n",
      "  0.2929889  0.6727449  0.71968585 0.8010821  0.41355345 0.91683173\n",
      "  0.8387901  0.6045625  0.8842177  0.8826764  0.25480294 0.5074297\n",
      "  0.8487877  0.24108186 0.90038013 0.42004842 0.7507496  0.82682496\n",
      "  0.96313024 0.445275   0.7538969  0.8853897  0.44888878 0.8641053\n",
      "  0.27045235 0.37754104 0.5932086  0.84132665 0.8154106  0.30761972\n",
      "  0.7204642  0.92849857 0.45925483 0.91614395 0.78438455 0.57453805\n",
      "  0.87220395 0.7056812  0.90794915 0.9854671  0.90975046 0.81758285\n",
      "  0.9807683  0.9888912  0.99123645 0.79706883 0.9304855  0.56574327\n",
      "  0.3646053  0.94114697 0.9322931  0.9603143  0.7784568  0.58973473\n",
      "  0.7924536  0.94791824 0.7095202  0.39544752 0.6771098  0.8434838\n",
      "  0.6051711  0.8523366  0.21053743 0.7822698  0.71769166 0.69406986\n",
      "  0.4851765  0.98365533 0.98760766 0.801135   0.858436   0.76043993\n",
      "  0.8986275  0.79017204 0.53074104 0.61446065 0.65697587 0.9587186\n",
      "  0.81116664 0.4987074  0.7209571  0.8171486  0.9454533  0.7813769\n",
      "  0.9524976  0.9112555  0.66190076 0.89619136 0.9651772  0.24486865\n",
      "  0.65369576 0.9558269  0.9033645  0.6244497  0.89265597 0.3480445\n",
      "  0.91517496 0.9794953  0.7174799  0.8004315  0.8671144  0.98788255\n",
      "  0.91528964 0.7170477  0.47784844 0.3987279  0.4801533  0.6656881\n",
      "  0.7383772  0.8727519 ]]\n",
      "0.9990609\n"
     ]
    }
   ],
   "source": [
    "block_size=(num_threads,1,1)\n",
    "grid_size=(1,1,1)\n",
    "kernel_function(b_g,a_g,np.int32(n),grid=grid_size,block=block_size)\n",
    "b_g.get(b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
